
# distributed training
deterministic: False
distributed: False
gpu: null
world_size: -1
rank: -1
dist_url: 'tcp://224.66.41.62:23456'
dist_backend: 'nccl'
multiprocessing_distributed: False
